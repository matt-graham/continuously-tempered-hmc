{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Force theano.scan to allow garbage collection to fix issue with memory leak\n",
    "os.environ['THEANO_FLAGS'] = 'scan.allow_gc=True'\n",
    "import time\n",
    "import theano as th\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from thermomc import discrete_temp\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set plot styling options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "sns.set(font='sans')\n",
    "sns.set_style('white', {\n",
    "    'font.family': 'sans',\n",
    "    'axes.labelcolor': '0.',\n",
    "    'text.color': '0.',\n",
    "    'xtick.color': '0.',\n",
    "    'ytick.color': '0.'\n",
    "})\n",
    "palette = sns.color_palette('husl', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(base_dir, 'data', 'radon')\n",
    "exp_dir = os.path.join(base_dir, 'experiments', 'pymc3-radon')\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "seed = 201702\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join(data_dir, 'radon-group.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional PyMC3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardLogistic(pm.distributions.Continuous):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(StandardLogistic, self).__init__(*args, **kwargs)\n",
    "        self.mean = self.mode = 0.\n",
    "\n",
    "    def random(self, point=None, size=None, repeat=None):\n",
    "        def _random(size=None):\n",
    "            return rng.logistic(size==size)\n",
    "\n",
    "        samples = generate_samples(_random,\n",
    "                                   dist_shape=self.shape,\n",
    "                                   broadcast_shape=mu.shape,\n",
    "                                   size=size)\n",
    "        return samples\n",
    "\n",
    "    def logp(self, value):\n",
    "        p = tt.nnet.sigmoid(value)\n",
    "        return tt.log(p) + tt.log(1. - p)\n",
    "\n",
    "class ExtendedModel(pm.Model):\n",
    "    \n",
    "    def __init__(self, target_model, base_means, base_stds, log_norm_est):\n",
    "        super(ExtendedModel, self).__init__()\n",
    "        self.named_vars.update(target_model.named_vars)\n",
    "        self.free_RVs += target_model.free_RVs\n",
    "        self.observed_RVs += target_model.observed_RVs\n",
    "        self.deterministics += target_model.deterministics\n",
    "        self.potentials  += target_model.potentials\n",
    "        self.missing_values += target_model.missing_values\n",
    "        self.target_model = target_model\n",
    "        self.base_means = base_means\n",
    "        self.base_stds = base_stds\n",
    "        self.log_norm_est = log_norm_est\n",
    "        with self:\n",
    "            temp_ctrl = StandardLogistic('temp_ctrl')\n",
    "        inv_temp = tt.nnet.sigmoid(temp_ctrl)\n",
    "        inv_temp.name = 'inv_temp'\n",
    "        delta = self.log_norm_est - self.target_model.logpt + self.base_logpt\n",
    "        prob_0 = tt.switch(tt.eq(delta, 0.), tt.ones_like(delta),\n",
    "                           -delta / tt.expm1(-delta))\n",
    "        prob_0.name = 'prob_0'\n",
    "        prob_1 = tt.switch(tt.eq(delta, 0.), tt.ones_like(delta),\n",
    "                           delta / tt.expm1(delta))\n",
    "        prob_1.name = 'prob_1'\n",
    "        self.deterministics += [inv_temp, prob_0, prob_1]\n",
    "        \n",
    "    @property\n",
    "    def base_logpt(self):\n",
    "        base_logp = 0\n",
    "        for cont_var in self.target_model.cont_vars:\n",
    "            mean = self.base_means[cont_var.name]\n",
    "            std = self.base_stds[cont_var.name]\n",
    "            base_logp -= tt.sum(\n",
    "                0.5 * ((cont_var - mean) / std)**2 + \n",
    "                0.5 * tt.log(2 * np.pi) + tt.log(std)\n",
    "            )\n",
    "        return base_logp\n",
    "    \n",
    "    @property\n",
    "    def logpt(self):\n",
    "        target_logp = self.target_model.logpt\n",
    "        base_logp = self.base_logpt\n",
    "        temp_ctrl = self.named_vars['temp_ctrl']\n",
    "        inv_temp = tt.nnet.sigmoid(temp_ctrl)\n",
    "        return (inv_temp * target_logp - inv_temp * self.log_norm_est + \n",
    "                (1 - inv_temp) * base_logp + temp_ctrl.logpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PyMC3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    sigma_alpha = pm.HalfCauchy('sigma_alpha', beta=2.5)\n",
    "    mu_alpha = pm.Normal('mu_alpha', mu=0, sd=1)\n",
    "    n_alpha = pm.MvNormal('n_alpha', mu=tt.zeros(data['n_counties']), \n",
    "                          chol=tt.eye(data['n_counties']), \n",
    "                          shape=data['n_counties'])\n",
    "    alpha = mu_alpha + sigma_alpha * n_alpha\n",
    "    sigma_beta = pm.HalfCauchy('sigma_beta', beta=2.5)\n",
    "    mu_beta = pm.Normal('mu_beta', mu=0, sd=1)\n",
    "    n_beta = pm.MvNormal('n_beta', mu=tt.zeros(2), chol=tt.eye(2), shape=2)\n",
    "    beta = mu_beta + sigma_beta * n_beta\n",
    "    epsilon = pm.HalfCauchy('epsilon', beta=2.5)\n",
    "    y_hat = alpha[data['county']] + data['x'] * beta[0] + data['u'] * beta[1]\n",
    "    y = pm.Normal('y', y_hat, epsilon, observed=data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit initial base density with ADVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advi_start_time = time.time()\n",
    "with model:\n",
    "    var_params = pm.variational.advi(\n",
    "        n=30000, accurate_elbo=False, \n",
    "        learning_rate=1e-3, tol_obj=1e-3, \n",
    "        random_seed=seed)\n",
    "advi_run_time = time.time() - advi_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_log_norm_est = var_params.elbo_vals[-100:].mean()\n",
    "print('Var log norm est={0:.2f}'.format(var_log_norm_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(os.path.join(exp_dir, 'advi-estimate.npz'), \n",
    "         var_log_norm_est=var_log_norm_est, advi_run_time=advi_run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_model = ExtendedModel(\n",
    "    model, var_params.means, var_params.stds, var_log_norm_est\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealed importance sampling for $\\log Z$ 'ground truth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually define energy functions (negative log unnormalised densities) for target and base distribution to allow use of separate Theano based annealed importance sampling implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_zeta = tt.constant(var_log_norm_est, name='log_zeta')\n",
    "\n",
    "def phi_func(x):\n",
    "    sigma_alpha_log = x[:, 0]\n",
    "    sigma_alpha = tt.exp(sigma_alpha_log)\n",
    "    mu_alpha = x[:, 1]\n",
    "    sigma_beta_log = x[:, 2]\n",
    "    sigma_beta = tt.exp(sigma_beta_log)\n",
    "    mu_beta = x[:, 3]\n",
    "    epsilon_log = x[:, 4]\n",
    "    epsilon = tt.exp(epsilon_log)\n",
    "    n_beta = x[:, 5:7]\n",
    "    n_alpha = x[:, 7:7 + data['n_counties']]\n",
    "    alpha = mu_alpha[:, None] + sigma_alpha[:, None] * n_alpha\n",
    "    beta = mu_beta[:, None] + sigma_beta[:, None] * n_beta\n",
    "    alpha.name = 'alpha'\n",
    "    beta.name = 'beta'\n",
    "    y_hat = (\n",
    "        alpha[:, data['county']] + \n",
    "        data['x'][None, :] * beta[:, 0][:, None] + \n",
    "        data['u'][None, :] * beta[:, 1][:, None]\n",
    "    )\n",
    "    return log_zeta + (\n",
    "        0.5 * (n_alpha**2).sum(-1) + 0.5 * data['n_counties'] * tt.log(2. * np.pi) +\n",
    "        0.5 * (n_beta**2).sum(-1) + 0.5 * 2 * tt.log(2. * np.pi) +\n",
    "        0.5 * (mu_alpha**2) + 0.5 * tt.log(2. * np.pi) +\n",
    "        0.5 * (mu_beta**2) + 0.5 * tt.log(2. * np.pi) +\n",
    "        tt.log1p((sigma_alpha / 2.5)**2) - tt.log(2) + \n",
    "        tt.log(np.pi) + tt.log(2.5) - tt.log(sigma_alpha) +\n",
    "        tt.log1p((sigma_beta / 2.5)**2) - tt.log(2) + \n",
    "        tt.log(np.pi) + tt.log(2.5) - tt.log(sigma_beta) +\n",
    "        tt.log1p((epsilon / 2.5)**2) - tt.log(2) + \n",
    "        tt.log(np.pi) + tt.log(2.5) - tt.log(epsilon) +\n",
    "        0.5 * (((data['y'][None, :] - y_hat) / epsilon[:, None])**2).sum(-1) + \n",
    "        0.5 * data['n_data'] * tt.log(2. * np.pi) +\n",
    "        data['n_data'] * tt.log(epsilon)\n",
    "    )\n",
    "\n",
    "def psi_func(x):\n",
    "    sigma_alpha_log_ = x[:, 0]\n",
    "    mu_alpha = x[:, 1]\n",
    "    sigma_beta_log_ = x[:, 2]\n",
    "    mu_beta = x[:, 3]\n",
    "    epsilon_log_ = x[:, 4]\n",
    "    n_beta = x[:, 5:7]\n",
    "    n_alpha = x[:, 7:7+data['n_counties']]\n",
    "    params = dict(\n",
    "        sigma_alpha_log_ = sigma_alpha_log_,\n",
    "        mu_alpha = mu_alpha,\n",
    "        sigma_beta_log_ = sigma_beta_log_,\n",
    "        mu_beta = mu_beta,\n",
    "        epsilon_log_ = epsilon_log_,\n",
    "        n_beta = n_beta,\n",
    "        n_alpha = n_alpha\n",
    "    )\n",
    "    psi = 0\n",
    "    for name, param in params.items():\n",
    "        mean = extended_model.base_means[name]\n",
    "        std = extended_model.base_stds[name]\n",
    "        if param.ndim == 2:\n",
    "            psi += (\n",
    "                0.5 * ((param - mean) / std)**2 + \n",
    "                0.5 * tt.log(2 * np.pi) + tt.log(std)\n",
    "            ).sum(-1)\n",
    "        else:\n",
    "            psi += (\n",
    "                0.5 * ((param - mean) / std)**2 + \n",
    "                0.5 * tt.log(2 * np.pi) + tt.log(std)\n",
    "            )\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_sampler = discrete_temp.AnnealedImportanceSampler(\n",
    "    tt.shared_randomstreams.RandomStreams(seed), False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = tt.matrix('pos')\n",
    "inv_temps = tt.vector('inv_temps')\n",
    "dt = tt.scalar('dt')\n",
    "n_step = tt.lscalar('n_step')\n",
    "hmc_params = {\n",
    "    'dt': dt,\n",
    "    'n_step': n_step,\n",
    "    'mom_resample_coeff': 1.,\n",
    "}\n",
    "pos_samples, log_weights, accepts, updates = ais_sampler.run(\n",
    "    pos, None, inv_temps, phi_func, psi_func, hmc_params)\n",
    "ais_chain_func = th.function(\n",
    "    [pos, inv_temps, dt, n_step],\n",
    "    [log_weights, accepts.mean()],\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def sigmoidal_schedule(num_temp, scale):\n",
    "    inv_temp_sched = sigmoid(\n",
    "        scale * (2. * np.arange(num_temp + 1) / num_temp - 1.))\n",
    "    return (\n",
    "        (inv_temp_sched - inv_temp_sched[0]) / \n",
    "        (inv_temp_sched[-1] - inv_temp_sched[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean = np.concatenate([\n",
    "    extended_model.base_means['sigma_alpha_log_'][None],\n",
    "    extended_model.base_means['mu_alpha'][None],\n",
    "    extended_model.base_means['sigma_beta_log_'][None],\n",
    "    extended_model.base_means['mu_beta'][None],\n",
    "    extended_model.base_means['epsilon_log_'][None],\n",
    "    extended_model.base_means['n_beta'],\n",
    "    extended_model.base_means['n_alpha']\n",
    "])\n",
    "var_std = np.concatenate([\n",
    "    extended_model.base_stds['sigma_alpha_log_'][None],\n",
    "    extended_model.base_stds['mu_alpha'][None],\n",
    "    extended_model.base_stds['sigma_beta_log_'][None],\n",
    "    extended_model.base_stds['mu_beta'][None],\n",
    "    extended_model.base_stds['epsilon_log_'][None],\n",
    "    extended_model.base_stds['n_beta'],\n",
    "    extended_model.base_stds['n_alpha']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_run = 100\n",
    "n_temp = 10000\n",
    "dt = 0.025\n",
    "n_step = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_temp_sched = sigmoidal_schedule(n_temp, 4.)\n",
    "n_init = rng.normal(size=(n_run, var_mean.shape[0]))\n",
    "pos_init = var_mean + n_init * var_std\n",
    "start_time = time.time()\n",
    "log_weights, accept = ais_chain_func(pos_init, inv_temp_sched, dt, n_step)\n",
    "ais_time = time.time() - start_time\n",
    "print('Accept={0:.2f} Time={1:.1f}s'.format(float(accept), ais_time))\n",
    "ais_log_norm_est = log_zeta.value + np.log(np.exp(log_weights).mean())\n",
    "print('AIS log norm est={0:.2f}'.format(ais_log_norm_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(exp_dir, 'ais-estimate.npz'), \n",
    "         ais_log_norm_est=ais_log_norm_est, log_weights=log_weights, ais_time=ais_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUTS run in extended space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng.seed(seed)\n",
    "num_reps = 10\n",
    "nuts_1_times = np.empty(num_reps) * np.nan\n",
    "nuts_1_traces = [None] * num_reps\n",
    "nuts_1_seeds = rng.randint(1000000, size=num_reps)\n",
    "scales = {'temp_ctrl': np.array(5.)}\n",
    "scales.update(var_params.stds)\n",
    "scales = extended_model.dict_to_array(scales)**2\n",
    "with model:\n",
    "    start = pm.variational.sample_vp(var_params, num_reps, random_seed=seed)\n",
    "for r in range(num_reps):\n",
    "    start[r]['temp_ctrl'] = rng.logistic()\n",
    "    with extended_model:\n",
    "        step = pm.NUTS(scaling=scales, is_cov=True)\n",
    "        start_time = time.time()\n",
    "        nuts_1_traces[r] = pm.sample(2500, step, start=start[r], init=None, \n",
    "                                     progressbar=True, tune=100, random_seed=nuts_1_seeds[r])\n",
    "        nuts_1_times[r] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, trace in enumerate(nuts_1_traces):\n",
    "    print('Chain {0}'.format(r))\n",
    "    print('  diverging={0}, mean accept={1:.2f}'.format(\n",
    "        trace.get_sampler_stats('diverging', combine=True).sum(),\n",
    "        trace.get_sampler_stats('mean_tree_accept', combine=True).mean(),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nuts_1_log_norm_ests = np.stack([\n",
    "    extended_model.log_norm_est + \n",
    "    np.log(np.array(nuts_1_traces[r].get_values('prob_1')).cumsum(0)) - \n",
    "    np.log(np.array(nuts_1_traces[r].get_values('prob_0')).cumsum(0))\n",
    "    for r in range(num_reps)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(exp_dir, 'intial-nuts-estimate.npz'), \n",
    "         log_norm_ests=nuts_1_log_norm_ests, run_time=nuts_1_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in nuts_1_traces:\n",
    "    _ = pm.plots.traceplot(trace, ['temp_ctrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in nuts_1_traces:\n",
    "    _ = pm.plots.traceplot(trace, ['inv_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "_ = sns.tsplot(\n",
    "    data=nuts_1_log_norm_ests, \n",
    "    time=advi_run_time + np.linspace(0, nuts_1_times.mean(), nuts_1_log_norm_ests.shape[-1]),\n",
    "    color=palette[2],\n",
    "    err_style=\"ci_band\", ci=[95], ax=ax, condition='CT NUTS', lw=1.5\n",
    ")\n",
    "ax.plot(np.linspace(0, advi_run_time, var_params.elbo_vals.shape[0] // 100), \n",
    "        var_params.elbo_vals.reshape(-1, 100).mean(-1), lw=1.)\n",
    "ax.plot([0, 60], [var_log_norm_est, var_log_norm_est], 'r-.', lw=1.)\n",
    "ax.plot([0, 60], [ais_log_norm_est, ais_log_norm_est], 'k--', lw=1.)\n",
    "ax.set_xlim(0, 60)\n",
    "ax.set_ylim(-1100, -1070)\n",
    "ax.plot([advi_run_time, advi_run_time], ax.get_ylim(), 'k:', lw=1.5)\n",
    "ax.set_xlabel('Time / s')\n",
    "ax.set_ylabel('Log marginal likelihood est.')\n",
    "ax.legend(['CT NUTS', 'ADVI', 'ADVI (final)', 'AIS'], loc='lower right', ncol=2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(exp_dir, 'hier-lin-regression-marg-lik.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update base density and $\\log \\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.seed(seed)\n",
    "extended_model_list = [None] * num_reps\n",
    "scales_list =[None] * num_reps\n",
    "base_params_list = [None] * num_reps\n",
    "start = [None] * num_reps\n",
    "for r, trace in enumerate(nuts_1_traces):\n",
    "    sum_prob_1 = trace.get_values('prob_1').sum()\n",
    "    sum_prob_0 = trace.get_values('prob_0').sum()\n",
    "    log_norm_est = extended_model.log_norm_est + np.log(sum_prob_1) - np.log(sum_prob_0)\n",
    "    sample_means = {\n",
    "        param: (trace.get_values('prob_1') * trace.get_values(param).T).sum(-1) \n",
    "        / sum_prob_1 \n",
    "        for param in var_params.means.keys()\n",
    "    }\n",
    "    sample_stds = {\n",
    "        param: ((trace.get_values('prob_1') * \n",
    "                (trace.get_values(param) - sample_means[param]).T**2).sum(-1) \n",
    "                / sum_prob_1)**0.5\n",
    "        for param in var_params.means.keys()\n",
    "    }\n",
    "    start[r] = pm.variational.sample_vp(\n",
    "        {'means': sample_means, 'stds': sample_stds}, 1, model=model, random_seed=nuts_1_seeds[r])\n",
    "    start[r][0]['temp_ctrl'] = rng.logistic()\n",
    "    extended_model_list[r] = ExtendedModel(\n",
    "        model, sample_means, sample_stds, log_norm_est\n",
    "    )\n",
    "    scales_list[r] = {'temp_ctrl': np.array(1.)}\n",
    "    scales_list[r].update(sample_stds)\n",
    "    scales_list[r] = extended_model_list[r].dict_to_array(scales_list[r])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final NUTS run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng.seed(seed)\n",
    "nuts_2_times = np.empty(num_reps) * np.nan\n",
    "nuts_2_traces = [None] * num_reps\n",
    "nuts_2_seeds = rng.randint(1000000, size=num_reps)\n",
    "scales = {'temp_ctrl': np.array(5.)}\n",
    "scales.update(var_params.stds)\n",
    "scales = extended_model.dict_to_array(scales)**2\n",
    "for r in range(num_reps):\n",
    "    with extended_model_list[r]:\n",
    "        step = pm.NUTS(scaling=scales_list[r], is_cov=True)\n",
    "        start_time = time.time()\n",
    "        nuts_2_traces[r] = pm.sample(5000, step, start=start[r][0], init=None, \n",
    "                                     progressbar=True, tune=500, random_seed=nuts_2_seeds[r])\n",
    "        nuts_2_times[r] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, trace in enumerate(nuts_2_traces):\n",
    "    print('Chain {0}'.format(r))\n",
    "    print('  diverging={0}, mean accept={1:.2f}'.format(\n",
    "        trace.get_sampler_stats('diverging', combine=True).sum(),\n",
    "        trace.get_sampler_stats('mean_tree_accept', combine=True).mean(),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nuts_2_log_norm_ests = np.stack([\n",
    "    extended_model.log_norm_est + \n",
    "    np.log(np.array(nuts_1_traces[r].get_values('prob_1')).cumsum(0)) - \n",
    "    np.log(np.array(nuts_1_traces[r].get_values('prob_0')).cumsum(0))\n",
    "    for r in range(num_reps)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(exp_dir, 'final-nuts-estimate.npz'), \n",
    "         log_norm_ests=nuts_2_log_norm_ests, run_time=nuts_2_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in nuts_2_traces:\n",
    "    _ = pm.plots.traceplot(trace, ['temp_ctrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in nuts_2_traces:\n",
    "    _ = pm.plots.traceplot(trace, ['inv_temp'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
